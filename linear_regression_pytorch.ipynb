{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction to Machine Learning Implementation with PyTorch\n",
    "\n",
    "*Most of the basic ideas are from [모두를 위한 머신러닝/딥러닝 강의](http://hunkim.github.io/ml/). All implementations posted can only be used for educational purposes with references to the original author, Sangjun Park. MS, Korea University, College of Medicine*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Brief overview of PyTorch\n",
    "\n",
    "PyTorch is an open-source python package explicitly developed for advanced research and application on deep learning. Pros when compared to Tensorflow are wide accessibility, intuitiveness of design, and __extreme dynamicity__ (which is the most fundamental difference between the two modules) when dealing with computation graphs. The only disadvantage regarding PyTorch is that compared to its superiority, it is still relatively a more recent product than Tensorflow, which makes it harder to gain much attention in most corporate settings. However, it is certain that the current paradigm is being shifted towards what we will discuss further, as more and more researchers are applying PyTorch as the main source of their works."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Installation, basic concepts and syntax documentations can be found at the [official pytorch website](https://pytorch.org/)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Linear Regression\n",
    "\n",
    "#### Single-variable Linear Regression\n",
    "\n",
    "The mathematical model for single-variable linear regression is as follows.\n",
    "\n",
    "$$y = \\beta_{0} + \\beta_{1}x$$\n",
    "\n",
    "We call $\\beta_{1}$ as the regression coefficient or the slope, and $\\beta_{0}$ as the intercept. This model is also called simple linear regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us first examine the exact solution using the least squares method. The method of least squares states that the sum of squares between the actual value and the fitted value, should be minimum. Using formal notations, minimization of $$(\\underset{\\text{Fitted value}}{\\hat{y}} - \\underset{\\text{Actual value}}{y})^2$$ is the goal of this model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The exact solution to this problem is the following whose derivations will be omitted. $$\\beta_{1} = \\frac{\\sum_{i=1}^{n}(x_{i} - \\bar{x})(y_{i} - \\bar{y})}{\\sum_{i=1}^{n}(x_{i} - \\bar{x})^{2}}\\; \\text{($\\bar{x}$ and $\\bar{y}$ are the mean values of x and y)}\\\\\\beta_{0} = \\bar{y} - \\beta_{1}\\bar{x}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this example, we will use the following data. $$x = \\begin{bmatrix}1.05&1.54&2.01&2.55&3.13&3.45&4.02&4.65&5.34&5.50\\end{bmatrix}^{T}$$ $$y = \\begin{bmatrix}10.3&15.4&21.2&25.4&30.1&35.6&40.5&43.4&50.1&56.6\\end{bmatrix}^{T}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7824239351471718 9.650293641652478\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(4, 30, '$y = 0.78 + 9.65x$')"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "x = np.array([1.05, 1.54, 2.01, 2.55, 3.13, 3.45, 4.02, 4.65, 5.34, 5.50])\n",
    "y = np.array([10.3, 15.4, 21.2, 25.4, 30.1, 35.6, 40.5, 43.4, 50.1, 56.6])\n",
    "\n",
    "b1 = np.sum((x - np.mean(x)) * (y - np.mean(y))) / np.sum((x - np.mean(x))**2)\n",
    "b0 = np.mean(y) - b1 * np.mean(x)\n",
    "\n",
    "x_range = np.linspace(0.5, 6.0, 1000)\n",
    "y_hat = b0 + b1 * x_range\n",
    "\n",
    "print(b0, b1)\n",
    "\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_range, y_hat, 'r')\n",
    "plt.text(4, 30, \"$y = 0.78 + 9.65x$\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, let's use the gradient descent method using PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "class linearRegression(torch.nn.Module):\n",
    "    def __init__(self, inputSize, outputSize):\n",
    "        super(linearRegression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(inputSize, outputSize, bias = True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.linear(x)\n",
    "        return out\n",
    "    \n",
    "inputDim = 1\n",
    "outputDim = 1\n",
    "learningRate = 0.001 \n",
    "epochs = 10000\n",
    "\n",
    "model = linearRegression(inputDim, outputDim)\n",
    "criterion = torch.nn.MSELoss() \n",
    "optimizer = torch.optim.SGD(model.parameters(), lr=learningRate)\n",
    "\n",
    "x_torch = torch.from_numpy(x).float().unsqueeze(dim=1)\n",
    "y_torch = torch.from_numpy(y).float().unsqueeze(dim=1)\n",
    "\n",
    "epoch_graph = np.array([])\n",
    "loss_graph = np.array([])\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    \n",
    "    inputs = x_torch\n",
    "    labels = y_torch\n",
    "\n",
    "    # Clear gradient buffers because we don't want any gradient from previous epoch to carry forward, dont want to cumulate gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # get output from the model with given inputs\n",
    "    outputs = model(inputs)\n",
    "\n",
    "    # get loss for the predicted output\n",
    "    loss = criterion(outputs, labels)\n",
    "    \n",
    "    # get gradients with respect to parameters\n",
    "    loss.backward()\n",
    "\n",
    "    # update parameters\n",
    "    optimizer.step()\n",
    "\n",
    "    epoch_graph = np.append(epoch_graph, epoch)\n",
    "    loss_graph = np.append(loss_graph, loss.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Loss in Log scale')"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.loglog(epoch_graph, loss_graph)\n",
    "plt.xlabel('Epoch in Log scale')\n",
    "plt.ylabel('Loss in Log scale')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameter containing:\n",
      "tensor([[9.6200]], requires_grad=True)\n",
      "Parameter containing:\n",
      "tensor([0.9013], requires_grad=True)\n"
     ]
    }
   ],
   "source": [
    "for param in model.parameters():\n",
    "    print(param)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Text(4, 30, '$y=0.9013+9.6200x$')"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_range2 = np.linspace(0.5, 5.5, 10)\n",
    "y_hat2 = model(torch.from_numpy(x_range2).float().unsqueeze(dim=1))\n",
    "plt.scatter(x, y)\n",
    "plt.plot(x_range2, y_hat2.detach().numpy(), 'r')\n",
    "plt.text(4, 30, \"$y=0.9013+9.6200x$\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
d>)\n",
      "epoch 7292, loss 2.30774188041687\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7293, loss 2.307732582092285\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7294, loss 2.307725429534912\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7295, loss 2.3077197074890137\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7296, loss 2.307711601257324\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7297, loss 2.307699203491211\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7298, loss 2.3076958656311035\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7299, loss 2.3076882362365723\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7300, loss 2.3076770305633545\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7301, loss 2.307671308517456\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7302, loss 2.3076653480529785\n",
      "tensor(2.3077, grad_fn=<MseLossBackward>)\n",
      "epoch 7303, loss 2.3076579570770264\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7304, loss 2.3076488971710205\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7305, loss 2.3076412677764893\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7306, loss 2.307635545730591\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7307, loss 2.307624101638794\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7308, loss 2.307619571685791\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7309, loss 2.3076119422912598\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7310, loss 2.307605028152466\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7311, loss 2.307594060897827\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7312, loss 2.3075897693634033\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7313, loss 2.307582378387451\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7314, loss 2.307572841644287\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7315, loss 2.3075664043426514\n",
      "tensor(2.3076, grad_fn=<MseLossBackward>)\n",
      "epoch 7316, loss 2.307555913925171\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7317, loss 2.307548999786377\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7318, loss 2.307542085647583\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7319, loss 2.3075356483459473\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7320, loss 2.307528257369995\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7321, loss 2.3075199127197266\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7322, loss 2.3075122833251953\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7323, loss 2.3075034618377686\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7324, loss 2.3074982166290283\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7325, loss 2.3074893951416016\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7326, loss 2.307480812072754\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7327, loss 2.3074755668640137\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7328, loss 2.307468891143799\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7329, loss 2.3074584007263184\n",
      "tensor(2.3075, grad_fn=<MseLossBackward>)\n",
      "epoch 7330, loss 2.30745267868042\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7331, loss 2.307446002960205\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7332, loss 2.307436943054199\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7333, loss 2.307429552078247\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7334, loss 2.307422399520874\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7335, loss 2.3074135780334473\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7336, loss 2.3074049949645996\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7337, loss 2.3074002265930176\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7338, loss 2.30739164352417\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7339, loss 2.3073840141296387\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7340, loss 2.307377338409424\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7341, loss 2.307370662689209\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7342, loss 2.307360887527466\n",
      "tensor(2.3074, grad_fn=<MseLossBackward>)\n",
      "epoch 7343, loss 2.3073525428771973\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7344, loss 2.307347536087036\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7345, loss 2.3073410987854004\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7346, loss 2.307330369949341\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7347, loss 2.3073248863220215\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7348, loss 2.307317018508911\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7349, loss 2.3073134422302246\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7350, loss 2.3073036670684814\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7351, loss 2.307295560836792\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7352, loss 2.3072891235351562\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7353, loss 2.307279109954834\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7354, loss 2.307274103164673\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7355, loss 2.307267189025879\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7356, loss 2.3072571754455566\n",
      "tensor(2.3073, grad_fn=<MseLossBackward>)\n",
      "epoch 7357, loss 2.307251453399658\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7358, loss 2.3072433471679688\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7359, loss 2.3072383403778076\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7360, loss 2.3072292804718018\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7361, loss 2.307220458984375\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7362, loss 2.3072147369384766\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7363, loss 2.3072071075439453\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7364, loss 2.3071980476379395\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7365, loss 2.3071913719177246\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7366, loss 2.307185173034668\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7367, loss 2.307178020477295\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7368, loss 2.307171583175659\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7369, loss 2.307162046432495\n",
      "tensor(2.3072, grad_fn=<MseLossBackward>)\n",
      "epoch 7370, loss 2.307155132293701\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7371, loss 2.307147979736328\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7372, loss 2.307137966156006\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7373, loss 2.307133197784424\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7374, loss 2.3071255683898926\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7375, loss 2.3071177005767822\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7376, loss 2.3071136474609375\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7377, loss 2.307103157043457\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7378, loss 2.3070969581604004\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7379, loss 2.30708909034729\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7380, loss 2.3070802688598633\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7381, loss 2.3070733547210693\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7382, loss 2.3070669174194336\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7383, loss 2.3070616722106934\n",
      "tensor(2.3071, grad_fn=<MseLossBackward>)\n",
      "epoch 7384, loss 2.3070549964904785\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7385, loss 2.3070461750030518\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7386, loss 2.307039976119995\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7387, loss 2.3070318698883057\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7388, loss 2.3070220947265625\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7389, loss 2.307015895843506\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7390, loss 2.3070106506347656\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7391, loss 2.307002544403076\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7392, loss 2.306997060775757\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7393, loss 2.3069870471954346\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7394, loss 2.3069818019866943\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7395, loss 2.306975841522217\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7396, loss 2.30696439743042\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7397, loss 2.306959629058838\n",
      "tensor(2.3070, grad_fn=<MseLossBackward>)\n",
      "epoch 7398, loss 2.3069517612457275\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7399, loss 2.3069446086883545\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7400, loss 2.306938648223877\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7401, loss 2.306929349899292\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7402, loss 2.3069233894348145\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7403, loss 2.3069164752960205\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7404, loss 2.306908369064331\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7405, loss 2.3069002628326416\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7406, loss 2.306894063949585\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7407, loss 2.3068859577178955\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7408, loss 2.3068814277648926\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7409, loss 2.306872606277466\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7410, loss 2.3068647384643555\n",
      "tensor(2.3069, grad_fn=<MseLossBackward>)\n",
      "epoch 7411, loss 2.3068594932556152\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7412, loss 2.306849718093872\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7413, loss 2.306844472885132\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7414, loss 2.306835889816284\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7415, loss 2.3068294525146484\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7416, loss 2.3068249225616455\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7417, loss 2.306814193725586\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7418, loss 2.3068089485168457\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7419, loss 2.3068015575408936\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7420, loss 2.3067924976348877\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7421, loss 2.3067851066589355\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7422, loss 2.3067798614501953\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7423, loss 2.306774139404297\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7424, loss 2.306767463684082\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7425, loss 2.3067586421966553\n",
      "tensor(2.3068, grad_fn=<MseLossBackward>)\n",
      "epoch 7426, loss 2.306751251220703\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7427, loss 2.306746006011963\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7428, loss 2.306736469268799\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7429, loss 2.306729555130005\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7430, loss 2.3067245483398438\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7431, loss 2.3067173957824707\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7432, loss 2.3067123889923096\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7433, loss 2.3067009449005127\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7434, loss 2.306694746017456\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7435, loss 2.306690216064453\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7436, loss 2.3066794872283936\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7437, loss 2.3066742420196533\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7438, loss 2.3066673278808594\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7439, loss 2.3066608905792236\n",
      "tensor(2.3067, grad_fn=<MseLossBackward>)\n",
      "epoch 7440, loss 2.306654453277588\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7441, loss 2.3066463470458984\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7442, loss 2.3066391944885254\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7443, loss 2.3066322803497314\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7444, loss 2.3066246509552\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7445, loss 2.3066163063049316\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7446, loss 2.3066115379333496\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7447, loss 2.306603193283081\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7448, loss 2.3065974712371826\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7449, loss 2.3065905570983887\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7450, loss 2.3065826892852783\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7451, loss 2.30657696723938\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7452, loss 2.3065667152404785\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7453, loss 2.30656099319458\n",
      "tensor(2.3066, grad_fn=<MseLossBackward>)\n",
      "epoch 7454, loss 2.306554079055786\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7455, loss 2.3065478801727295\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7456, loss 2.3065402507781982\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7457, loss 2.3065345287323\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7458, loss 2.306528091430664\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7459, loss 2.3065218925476074\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7460, loss 2.3065133094787598\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7461, loss 2.306507110595703\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7462, loss 2.3064990043640137\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7463, loss 2.3064920902252197\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7464, loss 2.306485652923584\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7465, loss 2.3064770698547363\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7466, loss 2.3064706325531006\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7467, loss 2.3064630031585693\n",
      "tensor(2.3065, grad_fn=<MseLossBackward>)\n",
      "epoch 7468, loss 2.306457996368408\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7469, loss 2.3064494132995605\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7470, loss 2.306443691253662\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7471, loss 2.3064377307891846\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7472, loss 2.3064327239990234\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7473, loss 2.306422472000122\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7474, loss 2.3064186573028564\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7475, loss 2.306408643722534\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7476, loss 2.306400775909424\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7477, loss 2.306398868560791\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7478, loss 2.3063900470733643\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7479, loss 2.306382656097412\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7480, loss 2.306375026702881\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7481, loss 2.3063693046569824\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7482, loss 2.306361675262451\n",
      "tensor(2.3064, grad_fn=<MseLossBackward>)\n",
      "epoch 7483, loss 2.306354284286499\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7484, loss 2.3063478469848633\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7485, loss 2.306340456008911\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7486, loss 2.3063344955444336\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7487, loss 2.3063292503356934\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7488, loss 2.306320905685425\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7489, loss 2.3063132762908936\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7490, loss 2.3063056468963623\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7491, loss 2.306299924850464\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7492, loss 2.3062922954559326\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7493, loss 2.3062877655029297\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7494, loss 2.3062806129455566\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7495, loss 2.306274890899658\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7496, loss 2.306269884109497\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7497, loss 2.3062586784362793\n",
      "tensor(2.3063, grad_fn=<MseLossBackward>)\n",
      "epoch 7498, loss 2.306251049041748\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7499, loss 2.306245803833008\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7500, loss 2.3062398433685303\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7501, loss 2.306232452392578\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7502, loss 2.3062264919281006\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7503, loss 2.3062188625335693\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7504, loss 2.3062119483947754\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7505, loss 2.3062052726745605\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7506, loss 2.3062002658843994\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7507, loss 2.306191921234131\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7508, loss 2.306184768676758\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7509, loss 2.3061773777008057\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7510, loss 2.306171417236328\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7511, loss 2.3061635494232178\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7512, loss 2.3061578273773193\n",
      "tensor(2.3062, grad_fn=<MseLossBackward>)\n",
      "epoch 7513, loss 2.3061509132385254\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7514, loss 2.306143045425415\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7515, loss 2.3061413764953613\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7516, loss 2.3061323165893555\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7517, loss 2.306126117706299\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7518, loss 2.3061184883117676\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7519, loss 2.306112766265869\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7520, loss 2.306105375289917\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7521, loss 2.3060975074768066\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7522, loss 2.306093692779541\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7523, loss 2.306086540222168\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7524, loss 2.306079387664795\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7525, loss 2.3060717582702637\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7526, loss 2.306063413619995\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7527, loss 2.306058168411255\n",
      "tensor(2.3061, grad_fn=<MseLossBackward>)\n",
      "epoch 7528, loss 2.3060505390167236\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7529, loss 2.306044340133667\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7530, loss 2.306037425994873\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7531, loss 2.3060333728790283\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7532, loss 2.306025981903076\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7533, loss 2.3060178756713867\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7534, loss 2.306011915206909\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7535, loss 2.306004762649536\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7536, loss 2.3059983253479004\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7537, loss 2.305989980697632\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7538, loss 2.30598521232605\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7539, loss 2.3059799671173096\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7540, loss 2.305972099304199\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7541, loss 2.30596661567688\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7542, loss 2.3059585094451904\n",
      "tensor(2.3060, grad_fn=<MseLossBackward>)\n",
      "epoch 7543, loss 2.30595064163208\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7544, loss 2.3059465885162354\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7545, loss 2.3059375286102295\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7546, loss 2.3059325218200684\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7547, loss 2.3059277534484863\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7548, loss 2.3059215545654297\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7549, loss 2.3059144020080566\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7550, loss 2.3059067726135254\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7551, loss 2.3059000968933105\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7552, loss 2.3058922290802\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7553, loss 2.305886745452881\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7554, loss 2.3058817386627197\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7555, loss 2.3058738708496094\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7556, loss 2.305868625640869\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7557, loss 2.305860757827759\n",
      "tensor(2.3059, grad_fn=<MseLossBackward>)\n",
      "epoch 7558, loss 2.3058555126190186\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7559, loss 2.305847644805908\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7560, loss 2.3058395385742188\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7561, loss 2.305833339691162\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7562, loss 2.3058266639709473\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7563, loss 2.3058228492736816\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7564, loss 2.3058152198791504\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7565, loss 2.3058090209960938\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7566, loss 2.305800676345825\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7567, loss 2.3057942390441895\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7568, loss 2.305788516998291\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7569, loss 2.3057806491851807\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7570, loss 2.305776834487915\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7571, loss 2.3057701587677\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7572, loss 2.3057639598846436\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7573, loss 2.3057587146759033\n",
      "tensor(2.3058, grad_fn=<MseLossBackward>)\n",
      "epoch 7574, loss 2.3057501316070557\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7575, loss 2.3057444095611572\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7576, loss 2.3057377338409424\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7577, loss 2.3057303428649902\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7578, loss 2.3057236671447754\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7579, loss 2.3057186603546143\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7580, loss 2.3057124614715576\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7581, loss 2.3057055473327637\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7582, loss 2.3056998252868652\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7583, loss 2.3056931495666504\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7584, loss 2.3056845664978027\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7585, loss 2.305678606033325\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7586, loss 2.3056745529174805\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7587, loss 2.3056674003601074\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7588, loss 2.305659770965576\n",
      "tensor(2.3057, grad_fn=<MseLossBackward>)\n",
      "epoch 7589, loss 2.3056530952453613\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7590, loss 2.305647373199463\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7591, loss 2.3056399822235107\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7592, loss 2.3056349754333496\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7593, loss 2.305626630783081\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7594, loss 2.305619478225708\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7595, loss 2.3056178092956543\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7596, loss 2.305607795715332\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7597, loss 2.3056023120880127\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7598, loss 2.3055949211120605\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7599, loss 2.3055899143218994\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7600, loss 2.3055825233459473\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7601, loss 2.305576801300049\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7602, loss 2.3055732250213623\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7603, loss 2.305565357208252\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7604, loss 2.3055598735809326\n",
      "tensor(2.3056, grad_fn=<MseLossBackward>)\n",
      "epoch 7605, loss 2.305551528930664\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7606, loss 2.305544137954712\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7607, loss 2.30553936958313\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7608, loss 2.305532217025757\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7609, loss 2.3055272102355957\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7610, loss 2.3055200576782227\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7611, loss 2.3055126667022705\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7612, loss 2.305506467819214\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7613, loss 2.3055012226104736\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7614, loss 2.3054943084716797\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7615, loss 2.3054871559143066\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7616, loss 2.305482864379883\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7617, loss 2.305476665496826\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7618, loss 2.305469274520874\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7619, loss 2.30546498298645\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7620, loss 2.3054566383361816\n",
      "tensor(2.3055, grad_fn=<MseLossBackward>)\n",
      "epoch 7621, loss 2.3054513931274414\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7622, loss 2.3054440021514893\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7623, loss 2.305438756942749\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7624, loss 2.3054325580596924\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7625, loss 2.305424213409424\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7626, loss 2.3054187297821045\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7627, loss 2.3054141998291016\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7628, loss 2.305405855178833\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7629, loss 2.3053996562957764\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7630, loss 2.305394411087036\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7631, loss 2.305389881134033\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7632, loss 2.305380344390869\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7633, loss 2.305373430252075\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7634, loss 2.3053689002990723\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7635, loss 2.3053641319274902\n",
      "tensor(2.3054, grad_fn=<MseLossBackward>)\n",
      "epoch 7636, loss 2.3053557872772217\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7637, loss 2.3053488731384277\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7638, loss 2.3053431510925293\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7639, loss 2.3053383827209473\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7640, loss 2.3053336143493652\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7641, loss 2.3053271770477295\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7642, loss 2.305321216583252\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7643, loss 2.3053131103515625\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7644, loss 2.305307149887085\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7645, loss 2.3053042888641357\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7646, loss 2.3052945137023926\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7647, loss 2.305288314819336\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7648, loss 2.305281400680542\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7649, loss 2.3052775859832764\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7650, loss 2.3052706718444824\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7651, loss 2.305264711380005\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7652, loss 2.3052570819854736\n",
      "tensor(2.3053, grad_fn=<MseLossBackward>)\n",
      "epoch 7653, loss 2.3052525520324707\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7654, loss 2.305245876312256\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7655, loss 2.3052382469177246\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7656, loss 2.3052334785461426\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7657, loss 2.3052291870117188\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7658, loss 2.305220365524292\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7659, loss 2.305215358734131\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7660, loss 2.3052093982696533\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7661, loss 2.3052048683166504\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7662, loss 2.3051955699920654\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7663, loss 2.305189847946167\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7664, loss 2.305184841156006\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7665, loss 2.305178642272949\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7666, loss 2.3051724433898926\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7667, loss 2.3051657676696777\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7668, loss 2.305159330368042\n",
      "tensor(2.3052, grad_fn=<MseLossBackward>)\n",
      "epoch 7669, loss 2.3051552772521973\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7670, loss 2.3051483631134033\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7671, loss 2.305142641067505\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7672, loss 2.3051352500915527\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7673, loss 2.3051302433013916\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7674, loss 2.3051235675811768\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7675, loss 2.3051161766052246\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7676, loss 2.3051116466522217\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7677, loss 2.3051064014434814\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7678, loss 2.305098056793213\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7679, loss 2.3050930500030518\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7680, loss 2.3050880432128906\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7681, loss 2.3050830364227295\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7682, loss 2.3050739765167236\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7683, loss 2.3050684928894043\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7684, loss 2.3050637245178223\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7685, loss 2.3050568103790283\n",
      "tensor(2.3051, grad_fn=<MseLossBackward>)\n",
      "epoch 7686, loss 2.3050506114959717\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7687, loss 2.3050453662872314\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7688, loss 2.3050386905670166\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7689, loss 2.3050339221954346\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7690, loss 2.305027484893799\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7691, loss 2.3050217628479004\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7692, loss 2.305014133453369\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7693, loss 2.305009603500366\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7694, loss 2.3050029277801514\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7695, loss 2.304995536804199\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7696, loss 2.3049910068511963\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7697, loss 2.3049864768981934\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7698, loss 2.3049778938293457\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7699, loss 2.3049728870391846\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7700, loss 2.3049681186676025\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7701, loss 2.3049628734588623\n",
      "tensor(2.3050, grad_fn=<MseLossBackward>)\n",
      "epoch 7702, loss 2.3049540519714355\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7703, loss 2.304948091506958\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7704, loss 2.304943561553955\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7705, loss 2.3049378395080566\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7706, loss 2.3049309253692627\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7707, loss 2.3049254417419434\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7708, loss 2.3049182891845703\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7709, loss 2.3049144744873047\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7710, loss 2.3049075603485107\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7711, loss 2.3049023151397705\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7712, loss 2.3048951625823975\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7713, loss 2.3048901557922363\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7714, loss 2.304884433746338\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7715, loss 2.3048763275146484\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7716, loss 2.3048720359802246\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7717, loss 2.3048672676086426\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7718, loss 2.304858922958374\n",
      "tensor(2.3049, grad_fn=<MseLossBackward>)\n",
      "epoch 7719, loss 2.3048534393310547\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7720, loss 2.3048489093780518\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7721, loss 2.3048441410064697\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 7722, loss 2.304835557937622\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7723, loss 2.3048293590545654\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7724, loss 2.3048248291015625\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7725, loss 2.304818630218506\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7726, loss 2.3048129081726074\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7727, loss 2.304806709289551\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7728, loss 2.3048007488250732\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7729, loss 2.3047962188720703\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7730, loss 2.3047900199890137\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7731, loss 2.304784059524536\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7732, loss 2.304777145385742\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7733, loss 2.3047726154327393\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7734, loss 2.3047657012939453\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7735, loss 2.3047585487365723\n",
      "tensor(2.3048, grad_fn=<MseLossBackward>)\n",
      "epoch 7736, loss 2.3047537803649902\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7737, loss 2.3047499656677246\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7738, loss 2.304741621017456\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7739, loss 2.304736614227295\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7740, loss 2.3047313690185547\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7741, loss 2.3047268390655518\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7742, loss 2.304718494415283\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7743, loss 2.3047127723693848\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7744, loss 2.3047072887420654\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7745, loss 2.304702043533325\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7746, loss 2.3046956062316895\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7747, loss 2.3046905994415283\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7748, loss 2.3046834468841553\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7749, loss 2.3046793937683105\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7750, loss 2.304673433303833\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7751, loss 2.3046674728393555\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7752, loss 2.3046600818634033\n",
      "tensor(2.3047, grad_fn=<MseLossBackward>)\n",
      "epoch 7753, loss 2.3046562671661377\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7754, loss 2.3046493530273438\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7755, loss 2.3046422004699707\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7756, loss 2.304638385772705\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7757, loss 2.304633617401123\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7758, loss 2.3046250343322754\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7759, loss 2.3046200275421143\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7760, loss 2.3046157360076904\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7761, loss 2.3046112060546875\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7762, loss 2.3046021461486816\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7763, loss 2.3045971393585205\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7764, loss 2.3045921325683594\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7765, loss 2.3045859336853027\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7766, loss 2.304579496383667\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7767, loss 2.304574489593506\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7768, loss 2.304567813873291\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7769, loss 2.3045642375946045\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7770, loss 2.3045570850372314\n",
      "tensor(2.3046, grad_fn=<MseLossBackward>)\n",
      "epoch 7771, loss 2.304553508758545\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7772, loss 2.3045458793640137\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7773, loss 2.3045387268066406\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7774, loss 2.304532527923584\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7775, loss 2.304530620574951\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7776, loss 2.3045241832733154\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7777, loss 2.304518222808838\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7778, loss 2.3045120239257812\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7779, loss 2.3045053482055664\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7780, loss 2.3044986724853516\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7781, loss 2.304495334625244\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7782, loss 2.3044888973236084\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7783, loss 2.3044826984405518\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7784, loss 2.3044772148132324\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7785, loss 2.3044705390930176\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7786, loss 2.3044662475585938\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7787, loss 2.3044593334198\n",
      "tensor(2.3045, grad_fn=<MseLossBackward>)\n",
      "epoch 7788, loss 2.3044543266296387\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7789, loss 2.304447889328003\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7790, loss 2.304441452026367\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7791, loss 2.304438829421997\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7792, loss 2.3044345378875732\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7793, loss 2.304425001144409\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7794, loss 2.3044190406799316\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7795, loss 2.304415225982666\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7796, loss 2.304408550262451\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7797, loss 2.3044028282165527\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7798, loss 2.3043975830078125\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7799, loss 2.3043923377990723\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7800, loss 2.3043880462646484\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7801, loss 2.3043813705444336\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7802, loss 2.304377555847168\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7803, loss 2.304368495941162\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7804, loss 2.30436372756958\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7805, loss 2.3043580055236816\n",
      "tensor(2.3044, grad_fn=<MseLossBackward>)\n",
      "epoch 7806, loss 2.304352283477783\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7807, loss 2.304347276687622\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7808, loss 2.3043437004089355\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7809, loss 2.304337978363037\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7810, loss 2.304332971572876\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7811, loss 2.3043274879455566\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7812, loss 2.3043200969696045\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7813, loss 2.3043148517608643\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7814, loss 2.3043100833892822\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7815, loss 2.3043034076690674\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7816, loss 2.3042969703674316\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7817, loss 2.304291248321533\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7818, loss 2.304286479949951\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7819, loss 2.3042805194854736\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7820, loss 2.3042759895324707\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7821, loss 2.3042702674865723\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7822, loss 2.3042666912078857\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7823, loss 2.304259777069092\n",
      "tensor(2.3043, grad_fn=<MseLossBackward>)\n",
      "epoch 7824, loss 2.3042514324188232\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7825, loss 2.304246425628662\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7826, loss 2.304241180419922\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7827, loss 2.3042328357696533\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7828, loss 2.304230213165283\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7829, loss 2.3042263984680176\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7830, loss 2.3042213916778564\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7831, loss 2.3042163848876953\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7832, loss 2.304211139678955\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7833, loss 2.304204225540161\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7834, loss 2.30419921875\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7835, loss 2.304191827774048\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7836, loss 2.3041868209838867\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7837, loss 2.3041818141937256\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7838, loss 2.3041751384735107\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7839, loss 2.3041718006134033\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7840, loss 2.3041634559631348\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7841, loss 2.3041586875915527\n",
      "tensor(2.3042, grad_fn=<MseLossBackward>)\n",
      "epoch 7842, loss 2.304154872894287\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7843, loss 2.3041484355926514\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7844, loss 2.304145097732544\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7845, loss 2.30413818359375\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7846, loss 2.3041317462921143\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7847, loss 2.3041281700134277\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7848, loss 2.3041229248046875\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7849, loss 2.3041136264801025\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7850, loss 2.304110050201416\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7851, loss 2.3041038513183594\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7852, loss 2.3041024208068848\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7853, loss 2.304095983505249\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7854, loss 2.3040897846221924\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7855, loss 2.304086208343506\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7856, loss 2.3040761947631836\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7857, loss 2.3040709495544434\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7858, loss 2.304068088531494\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7859, loss 2.304060459136963\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7860, loss 2.304055690765381\n",
      "tensor(2.3041, grad_fn=<MseLossBackward>)\n",
      "epoch 7861, loss 2.3040504455566406\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7862, loss 2.304044723510742\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7863, loss 2.3040413856506348\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7864, loss 2.304037094116211\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7865, loss 2.3040287494659424\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7866, loss 2.3040244579315186\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7867, loss 2.30401873588562\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7868, loss 2.3040122985839844\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7869, loss 2.3040077686309814\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7870, loss 2.3040027618408203\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7871, loss 2.30399751663208\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7872, loss 2.303990364074707\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7873, loss 2.3039846420288086\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7874, loss 2.303978681564331\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7875, loss 2.3039770126342773\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7876, loss 2.303971529006958\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7877, loss 2.3039658069610596\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7878, loss 2.303959846496582\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7879, loss 2.303955078125\n",
      "tensor(2.3040, grad_fn=<MseLossBackward>)\n",
      "epoch 7880, loss 2.303950548171997\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7881, loss 2.3039426803588867\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7882, loss 2.30393648147583\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7883, loss 2.3039326667785645\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7884, loss 2.303927421569824\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7885, loss 2.303922176361084\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7886, loss 2.3039193153381348\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7887, loss 2.3039112091064453\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7888, loss 2.303905725479126\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7889, loss 2.3039002418518066\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7890, loss 2.303894281387329\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7891, loss 2.3038902282714844\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7892, loss 2.3038840293884277\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7893, loss 2.3038787841796875\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7894, loss 2.30387544631958\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7895, loss 2.303868532180786\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7896, loss 2.3038651943206787\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7897, loss 2.3038578033447266\n",
      "tensor(2.3039, grad_fn=<MseLossBackward>)\n",
      "epoch 7898, loss 2.3038525581359863\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7899, loss 2.3038485050201416\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7900, loss 2.303842544555664\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7901, loss 2.303837299346924\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7902, loss 2.303833484649658\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7903, loss 2.3038268089294434\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7904, loss 2.3038201332092285\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7905, loss 2.3038151264190674\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7906, loss 2.303809404373169\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7907, loss 2.3038063049316406\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7908, loss 2.3037991523742676\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7909, loss 2.303795576095581\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7910, loss 2.3037917613983154\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7911, loss 2.3037853240966797\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7912, loss 2.3037805557250977\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7913, loss 2.3037731647491455\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7914, loss 2.3037686347961426\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7915, loss 2.3037631511688232\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7916, loss 2.303758144378662\n",
      "tensor(2.3038, grad_fn=<MseLossBackward>)\n",
      "epoch 7917, loss 2.303753614425659\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7918, loss 2.303748607635498\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7919, loss 2.303741931915283\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7920, loss 2.3037352561950684\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7921, loss 2.3037314414978027\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7922, loss 2.3037288188934326\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7923, loss 2.3037209510803223\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7924, loss 2.3037171363830566\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7925, loss 2.3037123680114746\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7926, loss 2.3037056922912598\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7927, loss 2.303701877593994\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7928, loss 2.303696870803833\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7929, loss 2.303689479827881\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7930, loss 2.3036842346191406\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7931, loss 2.303677797317505\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7932, loss 2.303676128387451\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7933, loss 2.303670883178711\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7934, loss 2.303664445877075\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7935, loss 2.303661823272705\n",
      "tensor(2.3037, grad_fn=<MseLossBackward>)\n",
      "epoch 7936, loss 2.303652763366699\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7937, loss 2.303647518157959\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7938, loss 2.3036465644836426\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7939, loss 2.303640842437744\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7940, loss 2.3036341667175293\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7941, loss 2.3036282062530518\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7942, loss 2.3036231994628906\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7943, loss 2.3036179542541504\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7944, loss 2.303612232208252\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7945, loss 2.3036084175109863\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7946, loss 2.3036046028137207\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7947, loss 2.3035988807678223\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7948, loss 2.3035941123962402\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7949, loss 2.3035879135131836\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7950, loss 2.3035831451416016\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7951, loss 2.3035755157470703\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7952, loss 2.3035728931427\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7953, loss 2.303565502166748\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7954, loss 2.3035638332366943\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7955, loss 2.303555965423584\n",
      "tensor(2.3036, grad_fn=<MseLossBackward>)\n",
      "epoch 7956, loss 2.303551197052002\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7957, loss 2.303544759750366\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7958, loss 2.303539752960205\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7959, loss 2.3035359382629395\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7960, loss 2.3035316467285156\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7961, loss 2.3035268783569336\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7962, loss 2.303520917892456\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7963, loss 2.3035149574279785\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7964, loss 2.3035101890563965\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7965, loss 2.3035061359405518\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7966, loss 2.3035013675689697\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7967, loss 2.303495168685913\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7968, loss 2.303490161895752\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7969, loss 2.303483486175537\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7970, loss 2.3034820556640625\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7971, loss 2.3034756183624268\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7972, loss 2.3034727573394775\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7973, loss 2.3034653663635254\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7974, loss 2.3034608364105225\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7975, loss 2.303457260131836\n",
      "tensor(2.3035, grad_fn=<MseLossBackward>)\n",
      "epoch 7976, loss 2.3034520149230957\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7977, loss 2.3034443855285645\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7978, loss 2.303441286087036\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7979, loss 2.303433656692505\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7980, loss 2.3034286499023438\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7981, loss 2.303424835205078\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7982, loss 2.303420066833496\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7983, loss 2.3034143447875977\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7984, loss 2.30340838432312\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7985, loss 2.3034045696258545\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7986, loss 2.3033995628356934\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7987, loss 2.3033955097198486\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7988, loss 2.3033909797668457\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7989, loss 2.303385019302368\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7990, loss 2.303382158279419\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7991, loss 2.3033766746520996\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7992, loss 2.3033719062805176\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7993, loss 2.303366184234619\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7994, loss 2.3033595085144043\n",
      "tensor(2.3034, grad_fn=<MseLossBackward>)\n",
      "epoch 7995, loss 2.303353786468506\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 7996, loss 2.303349018096924\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 7997, loss 2.3033435344696045\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 7998, loss 2.3033411502838135\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 7999, loss 2.303335428237915\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8000, loss 2.3033294677734375\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8001, loss 2.3033249378204346\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8002, loss 2.303318738937378\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8003, loss 2.303316831588745\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8004, loss 2.3033130168914795\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8005, loss 2.3033053874969482\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8006, loss 2.3033008575439453\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8007, loss 2.3032963275909424\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8008, loss 2.303288698196411\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8009, loss 2.303284168243408\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8010, loss 2.303278684616089\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8011, loss 2.3032753467559814\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8012, loss 2.303269863128662\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8013, loss 2.30326509475708\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8014, loss 2.3032615184783936\n",
      "tensor(2.3033, grad_fn=<MseLossBackward>)\n",
      "epoch 8015, loss 2.3032584190368652\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8016, loss 2.303248882293701\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8017, loss 2.3032467365264893\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8018, loss 2.30324125289917\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8019, loss 2.3032386302948\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8020, loss 2.3032329082489014\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8021, loss 2.303225040435791\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8022, loss 2.30322265625\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8023, loss 2.3032169342041016\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8024, loss 2.3032126426696777\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8025, loss 2.303205966949463\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8026, loss 2.303201913833618\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8027, loss 2.3031983375549316\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8028, loss 2.303192615509033\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8029, loss 2.3031864166259766\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8030, loss 2.3031840324401855\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8031, loss 2.303178548812866\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8032, loss 2.3031744956970215\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8033, loss 2.3031680583953857\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8034, loss 2.303161859512329\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8035, loss 2.3031582832336426\n",
      "tensor(2.3032, grad_fn=<MseLossBackward>)\n",
      "epoch 8036, loss 2.3031532764434814\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8037, loss 2.303147792816162\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8038, loss 2.30314564704895\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8039, loss 2.303138256072998\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8040, loss 2.3031327724456787\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8041, loss 2.303128242492676\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8042, loss 2.3031222820281982\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8043, loss 2.3031201362609863\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8044, loss 2.303114652633667\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8045, loss 2.3031094074249268\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8046, loss 2.3031060695648193\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8047, loss 2.3031017780303955\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8048, loss 2.3030953407287598\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8049, loss 2.303091049194336\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8050, loss 2.3030850887298584\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8051, loss 2.30308198928833\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8052, loss 2.3030762672424316\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8053, loss 2.3030714988708496\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8054, loss 2.303068161010742\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8055, loss 2.303062915802002\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch 8056, loss 2.303057909011841\n",
      "tensor(2.3031, grad_fn=<MseLossBackward>)\n",
      "epoch 8057, loss 2.303053617477417\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8058, loss 2.3030471801757812\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8059, loss 2.3030452728271484\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8060, loss 2.303039073944092\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8061, loss 2.303032159805298\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8062, loss 2.3030285835266113\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8063, loss 2.303022861480713\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8064, loss 2.303018093109131\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8065, loss 2.3030123710632324\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8066, loss 2.3030083179473877\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8067, loss 2.303004264831543\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8068, loss 2.3030006885528564\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8069, loss 2.3029944896698\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8070, loss 2.302992343902588\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8071, loss 2.3029863834381104\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8072, loss 2.3029818534851074\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8073, loss 2.3029751777648926\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8074, loss 2.302969455718994\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8075, loss 2.3029656410217285\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8076, loss 2.3029611110687256\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8077, loss 2.302955150604248\n",
      "tensor(2.3030, grad_fn=<MseLossBackward>)\n",
      "epoch 8078, loss 2.3029532432556152\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8079, loss 2.302947521209717\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8080, loss 2.3029415607452393\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8081, loss 2.3029370307922363\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8082, loss 2.3029332160949707\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8083, loss 2.3029308319091797\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8084, loss 2.3029239177703857\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8085, loss 2.302919864654541\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8086, loss 2.3029162883758545\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8087, loss 2.3029122352600098\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8088, loss 2.3029046058654785\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8089, loss 2.3028998374938965\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8090, loss 2.302894353866577\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8091, loss 2.3028922080993652\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8092, loss 2.302886962890625\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8093, loss 2.302882194519043\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8094, loss 2.3028783798217773\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8095, loss 2.302872896194458\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8096, loss 2.3028671741485596\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8097, loss 2.3028645515441895\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8098, loss 2.3028571605682373\n",
      "tensor(2.3029, grad_fn=<MseLossBackward>)\n",
      "epoch 8099, loss 2.3028550148010254\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8100, loss 2.302849769592285\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8101, loss 2.302842378616333\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8102, loss 2.302839756011963\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8103, loss 2.3028347492218018\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8104, loss 2.302830219268799\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8105, loss 2.3028242588043213\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8106, loss 2.302819013595581\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8107, loss 2.3028151988983154\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8108, loss 2.3028106689453125\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8109, loss 2.3028056621551514\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8110, loss 2.3028032779693604\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8111, loss 2.302797555923462\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8112, loss 2.3027937412261963\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8113, loss 2.3027877807617188\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8114, loss 2.302781343460083\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8115, loss 2.3027777671813965\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8116, loss 2.3027753829956055\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8117, loss 2.302769899368286\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8118, loss 2.3027663230895996\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8119, loss 2.302762746810913\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8120, loss 2.302755832672119\n",
      "tensor(2.3028, grad_fn=<MseLossBackward>)\n",
      "epoch 8121, loss 2.3027501106262207\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8122, loss 2.3027455806732178\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8123, loss 2.302743434906006\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8124, loss 2.302738666534424\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8125, loss 2.3027331829071045\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8126, loss 2.3027262687683105\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8127, loss 2.3027243614196777\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8128, loss 2.3027191162109375\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8129, loss 2.3027148246765137\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8130, loss 2.302711009979248\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8131, loss 2.302703380584717\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8132, loss 2.3027024269104004\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8133, loss 2.302696704864502\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8134, loss 2.3026912212371826\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8135, loss 2.302687406539917\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8136, loss 2.302682399749756\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8137, loss 2.3026769161224365\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8138, loss 2.302675724029541\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8139, loss 2.3026700019836426\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8140, loss 2.3026652336120605\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8141, loss 2.3026580810546875\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8142, loss 2.30265474319458\n",
      "tensor(2.3027, grad_fn=<MseLossBackward>)\n",
      "epoch 8143, loss 2.3026506900787354\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8144, loss 2.302645683288574\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8145, loss 2.302643060684204\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8146, loss 2.302638530731201\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8147, loss 2.3026318550109863\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8148, loss 2.3026270866394043\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8149, loss 2.3026247024536133\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8150, loss 2.302619457244873\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8151, loss 2.302614688873291\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8152, loss 2.302607536315918\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8153, loss 2.3026046752929688\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8154, loss 2.302600383758545\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8155, loss 2.3025970458984375\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8156, loss 2.302590847015381\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8157, loss 2.3025875091552734\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8158, loss 2.302584171295166\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8159, loss 2.3025786876678467\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8160, loss 2.302574872970581\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8161, loss 2.302568197250366\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8162, loss 2.302565097808838\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8163, loss 2.302563190460205\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8164, loss 2.302555799484253\n",
      "tensor(2.3026, grad_fn=<MseLossBackward>)\n",
      "epoch 8165, loss 2.3025503158569336\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8166, loss 2.3025460243225098\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8167, loss 2.3025431632995605\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8168, loss 2.302537441253662\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8169, loss 2.302534341812134\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8170, loss 2.302529811859131\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8171, loss 2.3025240898132324\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8172, loss 2.302520751953125\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8173, loss 2.302518367767334\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8174, loss 2.3025121688842773\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8175, loss 2.302506685256958\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8176, loss 2.3025004863739014\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8177, loss 2.3024988174438477\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8178, loss 2.30249285697937\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8179, loss 2.302487373352051\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8180, loss 2.302485704421997\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8181, loss 2.302478790283203\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8182, loss 2.3024742603302\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8183, loss 2.3024723529815674\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8184, loss 2.3024659156799316\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8185, loss 2.3024628162384033\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8186, loss 2.3024606704711914\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8187, loss 2.302452802658081\n",
      "tensor(2.3025, grad_fn=<MseLossBackward>)\n",
      "epoch 8188, loss 2.302450180053711\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8189, loss 2.3024439811706543\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8190, loss 2.302440643310547\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8191, loss 2.3024351596832275\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8192, loss 2.3024308681488037\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8193, loss 2.3024277687072754\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8194, loss 2.3024239540100098\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8195, loss 2.3024182319641113\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8196, loss 2.3024115562438965\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8197, loss 2.30241060256958\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8198, loss 2.3024072647094727\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8199, loss 2.3023998737335205\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8200, loss 2.302396297454834\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8201, loss 2.3023927211761475\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8202, loss 2.3023860454559326\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8203, loss 2.302384376525879\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8204, loss 2.302380084991455\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8205, loss 2.302373170852661\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8206, loss 2.30237078666687\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8207, loss 2.302366256713867\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8208, loss 2.302361249923706\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8209, loss 2.3023581504821777\n",
      "tensor(2.3024, grad_fn=<MseLossBackward>)\n",
      "epoch 8210, loss 2.3023524284362793\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8211, loss 2.3023486137390137\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8212, loss 2.302341938018799\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8213, loss 2.3023386001586914\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8214, loss 2.302335262298584\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8215, loss 2.3023297786712646\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8216, loss 2.3023273944854736\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8217, loss 2.30232310295105\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8218, loss 2.302316904067993\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8219, loss 2.302314281463623\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8220, loss 2.302311658859253\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8221, loss 2.302305221557617\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8222, loss 2.302299976348877\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8223, loss 2.302294969558716\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8224, loss 2.302293539047241\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8225, loss 2.3022868633270264\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8226, loss 2.302281618118286\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8227, loss 2.3022806644439697\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8228, loss 2.3022735118865967\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8229, loss 2.3022687435150146\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8230, loss 2.302267551422119\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8231, loss 2.302260160446167\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8232, loss 2.302257776260376\n",
      "tensor(2.3023, grad_fn=<MseLossBackward>)\n",
      "epoch 8233, loss 2.3022561073303223\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8234, loss 2.3022477626800537\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8235, loss 2.302243709564209\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8236, loss 2.302241563796997\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8237, loss 2.302237033843994\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8238, loss 2.3022305965423584\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8239, loss 2.3022265434265137\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8240, loss 2.3022236824035645\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8241, loss 2.3022193908691406\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8242, loss 2.3022139072418213\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8243, loss 2.302211046218872\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8244, loss 2.3022055625915527\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8245, loss 2.3022027015686035\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8246, loss 2.3021962642669678\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8247, loss 2.302191972732544\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8248, loss 2.3021888732910156\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8249, loss 2.302185535430908\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8250, loss 2.3021793365478516\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8251, loss 2.302175283432007\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8252, loss 2.302173137664795\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8253, loss 2.3021674156188965\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8254, loss 2.3021645545959473\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8255, loss 2.3021576404571533\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8256, loss 2.302155017852783\n",
      "tensor(2.3022, grad_fn=<MseLossBackward>)\n",
      "epoch 8257, loss 2.3021538257598877\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8258, loss 2.3021457195281982\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8259, loss 2.3021416664123535\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8260, loss 2.3021373748779297\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8261, loss 2.302133083343506\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8262, loss 2.302131175994873\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8263, loss 2.3021252155303955\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8264, loss 2.3021202087402344\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8265, loss 2.3021187782287598\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8266, loss 2.3021132946014404\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8267, loss 2.3021092414855957\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8268, loss 2.302105665206909\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8269, loss 2.302100419998169\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8270, loss 2.3020968437194824\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8271, loss 2.302089214324951\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8272, loss 2.3020873069763184\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8273, loss 2.30208420753479\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8274, loss 2.3020787239074707\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8275, loss 2.3020758628845215\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8276, loss 2.302069664001465\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8277, loss 2.3020663261413574\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8278, loss 2.302063226699829\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8279, loss 2.302058219909668\n",
      "tensor(2.3021, grad_fn=<MseLossBackward>)\n",
      "epoch 8280, loss 2.3020548820495605\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8281, loss 2.3020496368408203\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8282, loss 2.302044630050659\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8283, loss 2.3020434379577637\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8284, loss 2.302036762237549\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8285, loss 2.302031993865967\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8286, loss 2.302030563354492\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8287, loss 2.3020241260528564\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8288, loss 2.3020193576812744\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8289, loss 2.3020179271698\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8290, loss 2.302011251449585\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8291, loss 2.302008867263794\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8292, loss 2.3020033836364746\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8293, loss 2.30199933052063\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8294, loss 2.301995038986206\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8295, loss 2.3019912242889404\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8296, loss 2.301988124847412\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8297, loss 2.3019826412200928\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8298, loss 2.3019790649414062\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8299, loss 2.301975727081299\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8300, loss 2.301971912384033\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8301, loss 2.301966428756714\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8302, loss 2.3019633293151855\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8303, loss 2.30195951461792\n",
      "tensor(2.3020, grad_fn=<MseLossBackward>)\n",
      "epoch 8304, loss 2.3019540309906006\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8305, loss 2.301948308944702\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8306, loss 2.3019466400146484\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8307, loss 2.301940441131592\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8308, loss 2.3019373416900635\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8309, loss 2.3019344806671143\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8310, loss 2.301928997039795\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8311, loss 2.3019258975982666\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8312, loss 2.301921844482422\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8313, loss 2.3019192218780518\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8314, loss 2.301912784576416\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8315, loss 2.3019096851348877\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8316, loss 2.301905393600464\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8317, loss 2.3019022941589355\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8318, loss 2.3018975257873535\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8319, loss 2.3018908500671387\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8320, loss 2.3018863201141357\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8321, loss 2.3018832206726074\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8322, loss 2.301879405975342\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8323, loss 2.301877021789551\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8324, loss 2.3018710613250732\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8325, loss 2.301867961883545\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8326, loss 2.301865339279175\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8327, loss 2.3018617630004883\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8328, loss 2.3018548488616943\n",
      "tensor(2.3019, grad_fn=<MseLossBackward>)\n",
      "epoch 8329, loss 2.3018524646759033\n",
      "tensor(2.3018, grad_fn=<MseLossBackward>)\n",
      "epoch 8330, loss 2.3018462657928467\n",
      "tensor(2.3018, grad_fn=<MseLossBackward>)\n",
      "epoch 8331, loss 2.301842212677002\n",
      "tensor(2.3018, grad_fn=<MseLossBackward>)\n",
      "epoch 8332, loss 2.3018393516540527\n",
      "tensor(2.3018, grad_fn=<MseLossBackward
